{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n"
     ]
    }
   ],
   "source": [
    "print (\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from langchain_community.document_loaders import UnstructuredPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "#from langchain.schema.runnable import RunnablePassthrough #added by me\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "import ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Constants\n",
    "DOC_PATH = \"./data/BOI.pdf\"\n",
    "MODEL_NAME = \"llama3.2\"\n",
    "#MODEL_NAME = \"llama3.1\"\n",
    "#MODEL_NAME = \"phi3.5\"\n",
    "\n",
    "EMBEDDING_MODEL = \"nomic-embed-text\"\n",
    "VECTOR_STORE_NAME = \"simple-rag\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_pdf(doc_path):\n",
    "    \"\"\"Load PDF documents.\"\"\"\n",
    "    if os.path.exists(doc_path):\n",
    "        loader = UnstructuredPDFLoader(file_path=doc_path)\n",
    "        data = loader.load()\n",
    "        logging.info(\"PDF loaded successfully.\")\n",
    "        return data\n",
    "    else:\n",
    "        logging.error(f\"PDF file not found at path: {doc_path}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def split_documents(documents):\n",
    "    \"\"\"Split documents into smaller chunks.\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1200, chunk_overlap=300)\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    logging.info(\"Documents split into chunks.\")\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def create_vector_db(chunks):\n",
    "    \"\"\"Create a vector database from document chunks.\"\"\"\n",
    "    # Pull the embedding model if not already available\n",
    "    ollama.pull(EMBEDDING_MODEL)\n",
    "\n",
    "    vector_db = Chroma.from_documents(\n",
    "        documents=chunks,\n",
    "        embedding=OllamaEmbeddings(model=EMBEDDING_MODEL),\n",
    "        collection_name=VECTOR_STORE_NAME,\n",
    "    )\n",
    "    logging.info(\"Vector database created.\")\n",
    "    return vector_db\n",
    "\n",
    "\n",
    "def create_retriever(vector_db, llm):\n",
    "    \"\"\"Create a multi-query retriever.\"\"\"\n",
    "    QUERY_PROMPT = PromptTemplate(\n",
    "        input_variables=[\"question\"],\n",
    "        template=\"\"\"You are an AI language model assistant. Your task is to generate five\n",
    "different versions of the given user question to retrieve relevant documents from\n",
    "a vector database. By generating multiple perspectives on the user question, your\n",
    "goal is to help the user overcome some of the limitations of the distance-based\n",
    "similarity search. Provide these alternative questions separated by newlines.\n",
    "Original question: {question}\"\"\",\n",
    "    )\n",
    "\n",
    "    retriever = MultiQueryRetriever.from_llm(\n",
    "        vector_db.as_retriever(), llm, prompt=QUERY_PROMPT\n",
    "    )\n",
    "    logging.info(\"Retriever created.\")\n",
    "    return retriever\n",
    "\n",
    "\n",
    "def create_chain(retriever, llm):\n",
    "    \"\"\"Create the chain\"\"\"\n",
    "    # RAG prompt\n",
    "    template = \"\"\"Answer the question based ONLY on the following context:\n",
    "{context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "    chain = (\n",
    "        {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    logging.info(\"Chain created successfully.\")\n",
    "    return chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Load and process the PDF document\n",
    "    data = ingest_pdf(DOC_PATH)\n",
    "    if data is None:\n",
    "        return\n",
    "\n",
    "    # Split the documents into chunks\n",
    "    chunks = split_documents(data)\n",
    "\n",
    "    # Create the vector database\n",
    "    vector_db = create_vector_db(chunks)\n",
    "\n",
    "    # Initialize the language model\n",
    "    llm = ChatOllama(model=MODEL_NAME)\n",
    "\n",
    "    # Create the retriever\n",
    "    retriever = create_retriever(vector_db, llm)\n",
    "\n",
    "    # Create the chain with preserved syntax\n",
    "    chain = create_chain(retriever, llm)\n",
    "\n",
    "    # Example query\n",
    "    my_question = \"How to report BOI?\"\n",
    "   \n",
    "     # Get the response\n",
    "    #res = chain.invoke(input=question)\n",
    "\n",
    "\n",
    "    # Get the response\n",
    "    #res = chain.invoke({input:question})\n",
    "    #res = chain.invoke(input=(question))\n",
    "    #res = chain.invoke(input={\"question\": question})\n",
    "    #res = chain.invoke({'input':question})\n",
    "    #res = chain.invoke(question = question)\n",
    "    #res = chain.invoke({'question': question})\n",
    "    input_data = { 'question': my_question}\n",
    "    res = chain.invoke(input=input_data)\n",
    "\n",
    "    print(\"Response:\")\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:PDF loaded successfully.\n",
      "INFO:root:Documents split into chunks.\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/pull \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Vector database created.\n",
      "INFO:root:Retriever created.\n",
      "INFO:root:Chain created successfully.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'question' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m----> 2\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[19], line 26\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m my_question \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHow to report BOI?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     25\u001b[0m  \u001b[38;5;66;03m# Get the response\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m res \u001b[38;5;241m=\u001b[39m chain\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[43mquestion\u001b[49m)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Get the response\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m#res = chain.invoke({input:question})\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m#res = chain.invoke(input=(question))\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m#res = chain.invoke(question = question)\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m#res = chain.invoke({'question': question})\u001b[39;00m\n\u001b[0;32m     36\u001b[0m input_data \u001b[38;5;241m=\u001b[39m { \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m'\u001b[39m:my_question}\n",
      "\u001b[1;31mNameError\u001b[0m: name 'question' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
